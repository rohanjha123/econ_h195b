{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a968334-f64d-41ad-bcc8-471af9963408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/rohanjha/.pyenv/versions/3.11.1/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/rohanjha/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/rohanjha/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rohanjha/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in /Users/rohanjha/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "!pip install nltk\n",
    "import re\n",
    "import nltk \n",
    "import string\n",
    "from markdown import Markdown\n",
    "from io import StringIO\n",
    "\n",
    "# !pip install vaderSentiment\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afcaea3-022d-43da-a0e2-2f59aac4cd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(\"../data/reddit_scrape.csv.zip\") as z:\n",
    "    with z.open(\"reddit_scrape.csv\") as f:\n",
    "        reddit_df = pd.read_csv(f)\n",
    "\n",
    "reddit_df['Date'] = pd.to_datetime(reddit_df['Date'])    \n",
    "reddit_df.head()\n",
    "\n",
    "scrape_cols = [x for x in reddit_df.columns if 'craped' in x]\n",
    "for col in scrape_cols: # converts strings to lists\n",
    "    reddit_df[col] = reddit_df[col].apply(lambda x: eval(x) if type(x)==str else x)\n",
    "\n",
    "    \n",
    "reddit_df['game_thread_comments'] = [lst[1:] for lst in reddit_df.loc[:,'Game thread scraped']]\n",
    "reddit_df['post_game_thread_comments'] = [lst[1:] if type(lst)==list else lst for lst in reddit_df.loc[:,'Post game thread Scraped']]\n",
    "reddit_df['all comments'] =  reddit_df['game_thread_comments'] + reddit_df['post_game_thread_comments']\n",
    "reddit_df.loc[reddit_df['all comments'].isna(),'all comments'] = \\\n",
    "        [lst[1:] for lst in reddit_df.loc[reddit_df['all comments'].isna(),'Game thread scraped']]\n",
    "reddit_df.drop(columns=['game_thread_comments','post_game_thread_comments'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5cd93a-8f7e-4d24-860c-fc40400e153a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0006ca-7663-4be7-945f-e92e620f4ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Falcons fans: are you guys ready for 17 straight weeks of hearing how you fucked up the Super Bowl? I feel your pain. Sincerely, Seahawks fans everywhere.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unmark_element(element, stream=None):\n",
    "    if stream is None:\n",
    "        stream = StringIO()\n",
    "    if element.text:\n",
    "        stream.write(element.text)\n",
    "    for sub in element:\n",
    "        unmark_element(sub, stream)\n",
    "    if element.tail:\n",
    "        stream.write(element.tail)\n",
    "    return stream.getvalue()\n",
    "\n",
    "# patching Markdown\n",
    "Markdown.output_formats[\"plain\"] = unmark_element\n",
    "__md = Markdown(output_format=\"plain\")\n",
    "__md.stripTopLevelTags = False\n",
    "\n",
    "\n",
    "def unmark(text):\n",
    "    return __md.convert(text).replace(\" \\n\",\" \")\n",
    "\n",
    "text = reddit_df['Game thread scraped'][0][1][0]\n",
    "text = unmark(text).replace(\" \\n\",\" \")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e38f69c-8b6f-4ad2-9ad2-f8aeaefb3e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Falcons fans are you guys ready for 17 straight weeks of hearing how you fucked up the Super Bowl I feel your pain Sincerely Seahawks fans everywhere'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c070f9a-f7ac-464d-a47e-39dda36574fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SuperBowl is a great event.\n"
     ]
    }
   ],
   "source": [
    "def convert_super_bowl(text):\n",
    "    # Use regular expression to find and replace \"super bowl\" with \"superbowl\" (case-insensitive)\n",
    "    modified_text = re.sub(r'\\b(super)\\s+(bowl)\\b', r'\\1\\2', text, flags=re.IGNORECASE)\n",
    "    return modified_text\n",
    "\n",
    "# Example usage:\n",
    "original_text = \"The Super Bowl is a great event.\"\n",
    "converted_text = convert_super_bowl(original_text)\n",
    "print(converted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9cdcc3f-a51a-45f4-bc8d-74c0e93bd778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    text = unmark(sentence).replace(\" \\n\",\" \")\n",
    "    text = convert_super_bowl(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734e53f-ea78-4412-a212-09095197e9ec",
   "metadata": {},
   "source": [
    "# Using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b341a2a9-1003-4175-81a9-8acaacd3155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"...\")\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    " \n",
    "@retry(wait=wait_random_exponential(min=1, max=61), stop=stop_after_attempt(7))\n",
    "def sentiment_analysis(transcription):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"As an AI with expertise in language and emotion analysis, your task is to analyze the sentiment of the following comment. This comment was posted on the r/nfl subreddit after a regular season NFL match. Please consider the overall tone of the discussion, the emotion conveyed by the language used, and the context in which words and phrases are used. Use one word to indicate whether the sentiment is generally positive, negative, or neutral. Be careful about sarcastic comments, indicating neutral if you are unsure about the sentiment in a comment.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": clean_text(transcription)\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1\n",
    "    )\n",
    "    if response.choices[0].message.content.lower() == \"positive\":\n",
    "        return 1\n",
    "    elif response.choices[0].message.content.lower() == \"negative\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def list_analyser(lst):\n",
    "    return [sentiment_analysis(i[0]) for i in lst]\n",
    "    \n",
    "def prop_calculator(analysed_lst):\n",
    "    proportion_pos = analysed_lst.count(1) / len(analysed_lst)\n",
    "    proportion_neg = analysed_lst.count(-1) / len(analysed_lst)\n",
    "    return (proportion_pos, proportion_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b9264d-3885-4aa7-a770-35a634423139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = list_analyser(reddit_df['all comments'][0])\n",
    "# alpha[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ef9fdf7-a011-47b1-80a3-406dd39cff97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 0\n",
      "Scraped 50\n",
      "Scraped 100\n",
      "Scraped 150\n",
      "Scraped 200\n",
      "Scraped 250\n",
      "Scraped 300\n",
      "Scraped 350\n",
      "Scraped 400\n",
      "Scraped 450\n",
      "Scraped 500\n",
      "Scraped 550\n",
      "Scraped 600\n",
      "Scraped 650\n",
      "Scraped 700\n",
      "Scraped 750\n",
      "Scraped 800\n",
      "Scraped 850\n",
      "Scraped 900\n",
      "Scraped 950\n",
      "Scraped 1000\n",
      "Scraped 1050\n",
      "Scraped 1100\n",
      "Scraped 1150\n",
      "Scraped 1200\n",
      "Scraped 1250\n",
      "Scraped 1300\n",
      "Scraped 1350\n",
      "Scraped 1400\n",
      "Scraped 1450\n",
      "Scraped 1500\n",
      "Scraped 1550\n",
      "Scraped 1567; Finished\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'problematic_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# if index > 5 and all(problematic_rows[-5:] == np.arange(index-4,index+1)):\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#        break\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(reddit_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; Finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast 5 problematic rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblematic_rows[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'problematic_rows' is not defined"
     ]
    }
   ],
   "source": [
    "lists = [eval(x) if type(x) == str else x for x in list(pd.read_csv('progress.csv')['props'])]\n",
    "#problematic_rows = [i for i in range(len(props)) if type(props[i]) != tuple and np.isnan(props[i])]\n",
    "for index, row in reddit_df.iloc[len(lists):,:].iterrows():\n",
    "    if index % 50 == 0:\n",
    "        print(f\"Scraped {index}\")\n",
    "    try:\n",
    "        lists.append(list_analyser(row['all comments']))\n",
    "    except:\n",
    "        #problematic_rows.append(index)\n",
    "        lists.append(np.NaN)\n",
    "    pd.DataFrame({'lists': lists}).to_csv('progress.csv')\n",
    "    # if index > 5 and all(problematic_rows[-5:] == np.arange(index-4,index+1)):\n",
    "    #        break\n",
    "print(f\"Scraped {len(reddit_df)}; Finished\")\n",
    "print(f\"Last 5 problematic rows: {problematic_rows[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d0e7974-e062-4d37-829a-8dc91b731d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, 0, -1, -1, 1, -1, -1, 0, -1, -1, -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-1, 0, 1, -1, 0, 1, -1, 0, -1, -1, 1, -1, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 0, -1, 1, 1, -1, 1, -1, 0, 0, 1, 0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[1, 0, -1, 0, 0, -1, -1, -1, -1, 1, 1, 0, 0, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>1562</td>\n",
       "      <td>[-1, -1, 0, -1, -1, 0, -1, 1, -1, 0, -1, -1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>1563</td>\n",
       "      <td>[1, 1, -1, 1, 0, 0, 0, -1, 1, 0, 1, -1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>1564</td>\n",
       "      <td>[0, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1, 1, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>1565</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>1566</td>\n",
       "      <td>[-1, -1, 0, -1, -1, 1, -1, 0, 1, -1, -1, -1, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              lists\n",
       "0              0  [-1, -1, 0, -1, -1, 1, -1, -1, 0, -1, -1, -1, ...\n",
       "1              1  [-1, 0, 1, -1, 0, 1, -1, 0, -1, -1, 1, -1, -1,...\n",
       "2              2  [1, 0, -1, 1, 1, -1, 1, -1, 0, 0, 1, 0, 0, -1,...\n",
       "3              3  [1, 0, -1, 0, 0, -1, -1, -1, -1, 1, 1, 0, 0, -...\n",
       "4              4  [1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, ...\n",
       "...          ...                                                ...\n",
       "1562        1562  [-1, -1, 0, -1, -1, 0, -1, 1, -1, 0, -1, -1, 0...\n",
       "1563        1563  [1, 1, -1, 1, 0, 0, 0, -1, 1, 0, 1, -1, 1, 1, ...\n",
       "1564        1564  [0, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1, 1, -1,...\n",
       "1565        1565  [-1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 1, ...\n",
       "1566        1566  [-1, -1, 0, -1, -1, 1, -1, 0, 1, -1, -1, -1, 1...\n",
       "\n",
       "[1567 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('progress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9726b7a-ef2f-4e30-bf27-b026995cebbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 108, 163, 377, 428, 598, 647, 828, 944, 976, 1163]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists = [eval(x) if type(x) == str else x for x in list(pd.read_csv('progress.csv')['lists'])]\n",
    "problematic_rows = [i for i in range(len(lists)) if type(lists[i]) != list and np.isnan(lists[i])]\n",
    "problematic_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eca7c4-b469-4dec-a1db-f2d5bafbd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [eval(x) if type(x) == str else x for x in list(pd.read_csv('progress.csv')['lists'])]\n",
    "problematic_rows = [i for i in range(len(lists)) if type(lists[i]) != list and np.isnan(lists[i])]\n",
    "for i in problematic_rows:\n",
    "    try:\n",
    "        lists[i] = list_analyser(row['all comments'])\n",
    "    except:\n",
    "        lists[i] = np.NaN\n",
    "    pd.DataFrame({'lists': lists}).to_csv('progress.csv')\n",
    "print(f\"Scraped {len(problematic_rows)}; Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce971880-eddd-41d6-afaa-f3b908170918",
   "metadata": {},
   "source": [
    "# DO NOT CLOSE THE COMPUTER; A SCRIPT IS RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec87ad0-720e-44c0-9ab4-ed199671ebc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn-env]",
   "language": "python",
   "name": "conda-env-sklearn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
