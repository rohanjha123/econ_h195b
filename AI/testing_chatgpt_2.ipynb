{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d3221cc-0702-48ae-a0af-d0c290c8efc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/rohanjha/.pyenv/versions/3.11.1/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/rohanjha/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/rohanjha/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rohanjha/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in /Users/rohanjha/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "!pip install nltk\n",
    "import re\n",
    "import nltk \n",
    "import string\n",
    "from markdown import Markdown\n",
    "from io import StringIO\n",
    "\n",
    "# !pip install vaderSentiment\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e26855-6c39-492c-a89b-1e42926d2a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(\"../data/reddit_scrape.csv.zip\") as z:\n",
    "    with z.open(\"reddit_scrape.csv\") as f:\n",
    "        reddit_df = pd.read_csv(f)\n",
    "\n",
    "reddit_df['Date'] = pd.to_datetime(reddit_df['Date'])    \n",
    "reddit_df.head()\n",
    "\n",
    "scrape_cols = [x for x in reddit_df.columns if 'craped' in x]\n",
    "for col in scrape_cols: # converts strings to lists\n",
    "    reddit_df[col] = reddit_df[col].apply(lambda x: eval(x) if type(x)==str else x)\n",
    "\n",
    "    \n",
    "reddit_df['game_thread_comments'] = [lst[1:] for lst in reddit_df.loc[:,'Game thread scraped']]\n",
    "reddit_df['post_game_thread_comments'] = [lst[1:] if type(lst)==list else lst for lst in reddit_df.loc[:,'Post game thread Scraped']]\n",
    "reddit_df['all comments'] =  reddit_df['game_thread_comments'] + reddit_df['post_game_thread_comments']\n",
    "reddit_df.loc[reddit_df['all comments'].isna(),'all comments'] = \\\n",
    "        [lst[1:] for lst in reddit_df.loc[reddit_df['all comments'].isna(),'Game thread scraped']]\n",
    "reddit_df.drop(columns=['game_thread_comments','post_game_thread_comments'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce077a5-ebf1-45d3-b134-a03406c11573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Falcons fans: are you guys ready for 17 straight weeks of hearing how you fucked up the Super Bowl? I feel your pain. Sincerely, Seahawks fans everywhere.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unmark_element(element, stream=None):\n",
    "    if stream is None:\n",
    "        stream = StringIO()\n",
    "    if element.text:\n",
    "        stream.write(element.text)\n",
    "    for sub in element:\n",
    "        unmark_element(sub, stream)\n",
    "    if element.tail:\n",
    "        stream.write(element.tail)\n",
    "    return stream.getvalue()\n",
    "\n",
    "# patching Markdown\n",
    "Markdown.output_formats[\"plain\"] = unmark_element\n",
    "__md = Markdown(output_format=\"plain\")\n",
    "__md.stripTopLevelTags = False\n",
    "\n",
    "\n",
    "def unmark(text):\n",
    "    return __md.convert(text).replace(\" \\n\",\" \")\n",
    "\n",
    "text = reddit_df['Game thread scraped'][0][1][0]\n",
    "text = unmark(text).replace(\" \\n\",\" \")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5f2211-df46-4b1d-bcd8-aace92bac7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Falcons fans are you guys ready for 17 straight weeks of hearing how you fucked up the Super Bowl I feel your pain Sincerely Seahawks fans everywhere'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9327908b-1df2-4213-8c36-2dc256a35845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SuperBowl is a great event.\n"
     ]
    }
   ],
   "source": [
    "def convert_super_bowl(text):\n",
    "    # Use regular expression to find and replace \"super bowl\" with \"superbowl\" (case-insensitive)\n",
    "    modified_text = re.sub(r'\\b(super)\\s+(bowl)\\b', r'\\1\\2', text, flags=re.IGNORECASE)\n",
    "    return modified_text\n",
    "\n",
    "# Example usage:\n",
    "original_text = \"The Super Bowl is a great event.\"\n",
    "converted_text = convert_super_bowl(original_text)\n",
    "print(converted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de27866-5eea-4b4b-bfaf-ce5fffef6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    text = unmark(sentence).replace(\" \\n\",\" \")\n",
    "    text = convert_super_bowl(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a3e92b6-c749-49bb-8f6f-eb796745ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"...\")\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    " \n",
    "@retry(wait=wait_random_exponential(min=1, max=61), stop=stop_after_attempt(7))\n",
    "def sentiment_analysis(transcription):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"As an AI with expertise in language and emotion analysis, your task is to analyze the sentiment of the following comment. This comment was posted on the r/nfl subreddit after a regular season NFL match. Please consider the overall tone of the discussion, the emotion conveyed by the language used, and the context in which words and phrases are used. Use one word to indicate whether the sentiment is generally positive, negative, or neutral. Be careful about sarcastic comments, indicating neutral if you are unsure about the sentiment in a comment.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": transcription\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1\n",
    "    )\n",
    "    if response.choices[0].message.content == \"Positive\":\n",
    "        return 1\n",
    "    elif response.choices[0].message.content == \"Negative\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def list_analyser(lst):\n",
    "    return [sentiment_analysis(i[0]) for i in lst]\n",
    "    \n",
    "def prop_calculator(analysed_lst):\n",
    "    proportion_pos = analysed_lst.count(1) / len(analysed_lst)\n",
    "    proportion_neg = analysed_lst.count(-1) / len(analysed_lst)\n",
    "    return (proportion_pos, proportion_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32210f08-58e5-4b7e-9625-6821dcec9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"As an AI with expertise in language and emotion analysis, your task is to analyze the sentiment of the following comment. This comment was posted on the r/nfl subreddit after a regular season NFL match. Please consider the overall tone of the discussion, the emotion conveyed by the language used, and the context in which words and phrases are used. Use one word to indicate whether the sentiment is generally positive, negative, or neutral. Be careful about sarcastic comments, indicating neutral if you are unsure about the sentiment in a comment.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": clean_text(reddit_df['all comments'][0][1][0])\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d6e7a83-deea-44fd-a36f-330955778291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-97z3bfVaOKHxfXaZfD1GQCJ88WnDP', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Negative', role='assistant', function_call=None, tool_calls=None))], created=1711690027, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3bc1b5746c', usage=CompletionUsage(completion_tokens=1, prompt_tokens=139, total_tokens=140))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc4611f5-dea8-411d-9c6f-9bfeb3ceafad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis(clean_text(reddit_df['all comments'][0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "422ecf6d-5a4c-4fa1-ab57-78a76eb8fc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(pd.read_csv('progress_2.csv').iloc[:,0])#['props']\n",
    "#reddit_df['all comments'][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1aacc2-616c-4ea5-8415-47ea1accfa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 100\n"
     ]
    }
   ],
   "source": [
    "props = [eval(x) if type(x) == str else x for x in list(pd.read_csv('progress_2.csv')['props'])]\n",
    "#problematic_rows = [i for i in range(len(props)) if type(props[i]) != tuple and np.isnan(props[i])]\n",
    "for index, row in reddit_df[np.max(pd.read_csv('progress_2.csv').iloc[:,0]):].iterrows():\n",
    "    if index % 50 == 0:\n",
    "        print(f\"Scraped {index}\")\n",
    "    try:\n",
    "        props.append(prop_calculator(list_analyser(row['all comments'])))\n",
    "    except:\n",
    "        #problematic_rows.append(index)\n",
    "        props.append(np.NaN)\n",
    "    pd.DataFrame({'props': props}).to_csv('progress_2.csv')\n",
    "    # if index > 5 and all(problematic_rows[-5:] == np.arange(index-4,index+1)):\n",
    "    #        break\n",
    "print(f\"Scraped {len(reddit_df)}; Finished\")\n",
    "print(f\"Last 5 problematic rows: {problematic_rows[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d4fe5-4b04-4157-ad72-e51a75dfe38f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn-env]",
   "language": "python",
   "name": "conda-env-sklearn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
