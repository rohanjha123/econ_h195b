{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "314e4aeb-d1ed-4f02-bafb-0804c10a2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "!pip install nltk\n",
    "import re\n",
    "import nltk \n",
    "import string\n",
    "from markdown import Markdown\n",
    "from io import StringIO\n",
    "\n",
    "!pip install vaderSentiment\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebfc366a-4ca6-4a07-a536-403b77eab596",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"...\")\n",
    "\n",
    "# text = \"I am bullish on $AAPL\"\n",
    "# response = client.completions.create(model=\"gpt-3.5-turbo\",\n",
    "# prompt=(f\"Sentiment analysis of the following text: '{text}'\\n\\nSentiment score: \"),\n",
    "# temperature=0,\n",
    "# max_tokens=1)\n",
    "\n",
    "# sentiment = response.choices[0].text.strip()\n",
    "# print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f323a7e8-f52b-41da-ab23-6a60e4a43b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"As an AI with expertise in language and emotion analysis, your task is to analyze the sentiment of the following comment. This comment was posted on the r/nfl subreddit after a regular season NFL match. Please consider the overall tone of the discussion, the emotion conveyed by the language used, and the context in which words and phrases are used. Use one word to indicate whether the sentiment is generally positive, negative, or neutral.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": clean_text(texts[0])\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "953d1f9a-5d93-4a4d-a4dd-1ab096759c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-979t4oSZJzFu7WiL0bripWaUeX02J', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Positive', role='assistant', function_call=None, tool_calls=None))], created=1711493330, model='gpt-4-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=105, total_tokens=106))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "78d78c2f-afdd-44f3-af7a-5570dd33d22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ") \n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def sentiment_analysis(transcription):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"As an AI with expertise in language and emotion analysis, your task is to analyze the sentiment of the following comment. This comment was posted on the r/nfl subreddit after a regular season NFL match. Please consider the overall tone of the discussion, the emotion conveyed by the language used, and the context in which words and phrases are used. Use one word to indicate whether the sentiment is generally positive, negative, or neutral.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": transcription\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "sentiment_analysis(\"I don't want to hear another pathetic word about how much the Packers are getting \\\"helped,\\\" get over yourselves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7ad30a3-c304-4361-b96c-fc7d5fae4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcription = \"I don't want to hear another pathetic word about how much the Packers are getting \\\"helped,\\\" get over yourselves\"\n",
    "# response = client.chat.completions.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         temperature=0,\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": \"As an AI with expertise in language and emotion analysis, your task is to analyze the sentiment of the following comment. This comment was posted on Reddit after a regular season NFL match. Please consider the overall tone of the discussion, the emotion conveyed by the language used, and the context in which words and phrases are used. Use one word to indicate whether the sentiment is generally positive, negative, or neutral.\"\n",
    "#             },\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": transcription\n",
    "#             }\n",
    "#         ],\n",
    "#         max_tokens=1\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5516c8b6-fc67-4cbb-8a63-f76f5c1d83fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b1336-0e00-4e62-acfc-33f05faee587",
   "metadata": {},
   "source": [
    "# Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2910c083-6863-44fe-a01c-76b15f1ca279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(\"../data/reddit_scrape.csv.zip\") as z:\n",
    "    with z.open(\"reddit_scrape.csv\") as f:\n",
    "        reddit_df = pd.read_csv(f)\n",
    "\n",
    "reddit_df['Date'] = pd.to_datetime(reddit_df['Date'])    \n",
    "reddit_df.head()\n",
    "\n",
    "scrape_cols = [x for x in reddit_df.columns if 'craped' in x]\n",
    "for col in scrape_cols: # converts strings to lists\n",
    "    reddit_df[col] = reddit_df[col].apply(lambda x: eval(x) if type(x)==str else x)\n",
    "\n",
    "    \n",
    "reddit_df['game_thread_comments'] = [lst[1:] for lst in reddit_df.loc[:,'Game thread scraped']]\n",
    "reddit_df['post_game_thread_comments'] = [lst[1:] if type(lst)==list else lst for lst in reddit_df.loc[:,'Post game thread Scraped']]\n",
    "reddit_df['all comments'] =  reddit_df['game_thread_comments'] + reddit_df['post_game_thread_comments']\n",
    "reddit_df.loc[reddit_df['all comments'].isna(),'all comments'] = \\\n",
    "        [lst[1:] for lst in reddit_df.loc[reddit_df['all comments'].isna(),'Game thread scraped']]\n",
    "reddit_df.drop(columns=['game_thread_comments','post_game_thread_comments'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8402c2ff-b5bd-447f-82ec-f50377373fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Falcons fans: are you guys ready for 17 straight weeks of hearing how you fucked up the Super Bowl? I feel your pain. Sincerely, Seahawks fans everywhere.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unmark_element(element, stream=None):\n",
    "    if stream is None:\n",
    "        stream = StringIO()\n",
    "    if element.text:\n",
    "        stream.write(element.text)\n",
    "    for sub in element:\n",
    "        unmark_element(sub, stream)\n",
    "    if element.tail:\n",
    "        stream.write(element.tail)\n",
    "    return stream.getvalue()\n",
    "\n",
    "\n",
    "# patching Markdown\n",
    "Markdown.output_formats[\"plain\"] = unmark_element\n",
    "__md = Markdown(output_format=\"plain\")\n",
    "__md.stripTopLevelTags = False\n",
    "\n",
    "\n",
    "def unmark(text):\n",
    "    return __md.convert(text).replace(\" \\n\",\" \")\n",
    "\n",
    "text = reddit_df['Game thread scraped'][0][1][0]\n",
    "text = unmark(text).replace(\" \\n\",\" \")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e46cc46f-1de2-4852-8b2c-81288c9c95c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Falcons fans are you guys ready for 17 straight weeks of hearing how you fucked up the Super Bowl I feel your pain Sincerely Seahawks fans everywhere'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c44aabd2-2b33-4120-ad77-d2767b8efe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/rohanjha/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f37454fe-1a53-4b54-8eb3-ca01f16f8851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SuperBowl is a great event.\n"
     ]
    }
   ],
   "source": [
    "def convert_super_bowl(text):\n",
    "    # Use regular expression to find and replace \"super bowl\" with \"superbowl\" (case-insensitive)\n",
    "    modified_text = re.sub(r'\\b(super)\\s+(bowl)\\b', r'\\1\\2', text, flags=re.IGNORECASE)\n",
    "    return modified_text\n",
    "\n",
    "# Example usage:\n",
    "original_text = \"The Super Bowl is a great event.\"\n",
    "converted_text = convert_super_bowl(original_text)\n",
    "print(converted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "467d4b21-b9c7-42f7-97f9-247e40b7f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    text = unmark(sentence).replace(\" \\n\",\" \")\n",
    "    text = convert_super_bowl(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e4d470e8-34be-45ed-9728-7ec37872191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(sentence):\n",
    "    print(sentence)\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    print(\"________________________\")\n",
    "    text = unmark(sentence).replace(\" \\n\",\" \")\n",
    "    text = convert_super_bowl(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    tokenized_sentence = nltk.word_tokenize(text)\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    pos_word_list=[]\n",
    "    neu_word_list=[]\n",
    "    neg_word_list=[]\n",
    "\n",
    "    for word in tokenized_sentence:\n",
    "        if (sid_obj.polarity_scores(word)['compound']) >= 0.05:\n",
    "            pos_word_list.append(word)\n",
    "        elif (sid_obj.polarity_scores(word)['compound']) <= -0.05:\n",
    "            neg_word_list.append(word)\n",
    "        else:\n",
    "            neu_word_list.append(word)                \n",
    "\n",
    "    print('Positive:', pos_word_list) \n",
    "    # pos_words.append(pos_word_list)\n",
    "    print('Neutral:', neu_word_list) \n",
    "    # neu_words.append(neu_word_list)\n",
    "    print('Negative:', neg_word_list) \n",
    "    # neg_words.append(neg_word_list)\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # oject gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    print(\"________________________\")\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "    print(\"Sentence Overall Rated As\", end = \" \")\n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        print(\"Positive\")\n",
    "        return \"Positive\"\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        print(\"Negative\")\n",
    "        return \"Negative\"\n",
    "    else :\n",
    "        print(\"Neutral\")\n",
    "        return \"Neutral\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5166c26d-0d79-429e-84c8-6ee1de1802f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [3193, (Dear Falcons fans: are you guys ready ...\n",
       "1       [109, (I'm a Jets fan. I'm a Texas A&M fan. Ye...\n",
       "2       [321, (Wow this national anthem singer is a fu...\n",
       "3       [192, (Man I hope Woodhead stays healthy for t...\n",
       "4       [367, (1 drive no turnovers. WE'VE TURNED THE ...\n",
       "                              ...                        \n",
       "1562    [58, (As long as the Cowboys lose everything i...\n",
       "1563    [46, (Amazon next gen stats showing that AJ re...\n",
       "1564    [115, (There's more Lions fans in here than Ra...\n",
       "1565    [49, (The \"bad throw PI\" is such fucking bulls...\n",
       "1566    [1158, (I feel like the NBC pregame show is ba...\n",
       "Name: Game thread scraped, Length: 1567, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['Game thread scraped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7fa091b9-3b73-4b8a-b97e-a771e24d13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_1 = random.randrange(len(reddit_df))\n",
    "num_2 = random.randrange(1,20)\n",
    "text = reddit_df['Game thread scraped'][num_1][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "179d16b6-66a5-4184-b825-c829c84246a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOLY FUCK, THIS GUY COULDN'T HIT ANYTHING IN A TAMPA JERSEY WHEN HE PLAYED FOR THEM NOW HE CAN'T FUCKING MISS.\n",
      "________________________\n",
      "Positive: ['PLAYED']\n",
      "Neutral: ['HOLY', 'THIS', 'GUY', 'COULDNT', 'HIT', 'ANYTHING', 'IN', 'A', 'TAMPA', 'JERSEY', 'WHEN', 'HE', 'FOR', 'THEM', 'NOW', 'HE', 'CANT', 'FUCKING']\n",
      "Negative: ['FUCK', 'MISS']\n",
      "________________________\n",
      "Overall sentiment dictionary is :  {'neg': 0.143, 'neu': 0.692, 'pos': 0.165, 'compound': -0.1127}\n",
      "sentence was rated as  14.299999999999999 % Negative\n",
      "sentence was rated as  69.19999999999999 % Neutral\n",
      "sentence was rated as  16.5 % Positive\n",
      "Sentence Overall Rated As Negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "849cba2e-001c-48ea-8226-984f10bef2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c7ecc4-9767-47d7-822c-759273deb5e9",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "87ced9e7-cbf1-4964-9fc1-611f5c076755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "texts = []\n",
    "vader_score = []\n",
    "gpt_score = []\n",
    "for i in range(1000):\n",
    "    num_1 = random.randrange(len(reddit_df))\n",
    "    num_2 = random.randrange(1,20)\n",
    "    text = reddit_df['Game thread scraped'][num_1][num_2][0]\n",
    "    texts.append(text)\n",
    "    text = clean_text(text)\n",
    "    blockPrint()\n",
    "    vader_score.append(sentiment_scores(text))\n",
    "    enablePrint()\n",
    "    gpt_score.append(sentiment_analysis(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6f59c9ee-1b38-4320-b02f-2a0d9cea19dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texts</th>\n",
       "      <th>vader</th>\n",
       "      <th>gpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Purdy-Kittle connection is something special.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Super bad play... That could have broken his neck</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SCARY TERRY HALLLOWEEN SPECIAL BITCHES</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This would be a great game in like any other s...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CRIPPLE FIIIIIIIGHT</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Aight peace y’all ✌️</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>JASON NOOO</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Everyone: We are down going into the half, we ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Which Seahawks come out today?\\nNeck and neck ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Texts     vader       gpt\n",
       "0    The Purdy-Kittle connection is something special.  Positive  Positive\n",
       "1    Super bad play... That could have broken his neck  Negative  Negative\n",
       "2               SCARY TERRY HALLLOWEEN SPECIAL BITCHES  Negative  Positive\n",
       "3    This would be a great game in like any other s...  Positive  Negative\n",
       "4                                  CRIPPLE FIIIIIIIGHT   Neutral  Negative\n",
       "..                                                 ...       ...       ...\n",
       "995                               Aight peace y’all ✌️  Positive   Neutral\n",
       "996                                         JASON NOOO   Neutral  Negative\n",
       "997  Everyone: We are down going into the half, we ...  Positive  Positive\n",
       "998  Which Seahawks come out today?\\nNeck and neck ...  Negative  Negative\n",
       "999                                          [deleted]   Neutral   Neutral\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(data=texts,columns=['Texts'])\n",
    "test_df['vader'] = vader_score\n",
    "test_df['gpt'] = gpt_score\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c7931f86-de26-4adb-9e8c-1cb05ad3a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"gpt_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e8e4fa6d-3a7a-423f-be5a-8e9ac2676efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(\"../data/reddit_scrape.csv.zip\") as z:\n",
    "    with z.open(\"reddit_scrape.csv\") as f:\n",
    "        reddit_df = pd.read_csv(f)\n",
    "\n",
    "reddit_df['Date'] = pd.to_datetime(reddit_df['Date'])    \n",
    "reddit_df.head()\n",
    "\n",
    "scrape_cols = [x for x in reddit_df.columns if 'craped' in x]\n",
    "for col in scrape_cols: # converts strings to lists\n",
    "    reddit_df[col] = reddit_df[col].apply(lambda x: eval(x) if type(x)==str else x)\n",
    "\n",
    "    \n",
    "reddit_df['game_thread_comments'] = [lst[1:] for lst in reddit_df.loc[:,'Game thread scraped']]\n",
    "reddit_df['post_game_thread_comments'] = [lst[1:] if type(lst)==list else lst for lst in reddit_df.loc[:,'Post game thread Scraped']]\n",
    "reddit_df['all comments'] =  reddit_df['game_thread_comments'] + reddit_df['post_game_thread_comments']\n",
    "reddit_df.loc[reddit_df['all comments'].isna(),'all comments'] = \\\n",
    "        [lst[1:] for lst in reddit_df.loc[reddit_df['all comments'].isna(),'Game thread scraped']]\n",
    "reddit_df.drop(columns=['game_thread_comments','post_game_thread_comments'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec1316-7094-4f1f-8e66-df0876d4d934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn-env]",
   "language": "python",
   "name": "conda-env-sklearn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
