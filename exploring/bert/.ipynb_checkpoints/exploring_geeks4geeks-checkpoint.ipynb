{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0b9c34c-5f6f-4843-8f15-ff81c37b8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e5be384-110e-4b9f-958e-98a948dc4040",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages/keras/utils/data_utils.py:179\u001b[0m, in \u001b[0;36m_extract_archive\u001b[0;34m(file_path, path, archive_format)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;66;03m# Tar archive, perhaps unsafe. Filter paths.\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m         archive\u001b[38;5;241m.\u001b[39mextractall(\n\u001b[1;32m    180\u001b[0m             path, members\u001b[38;5;241m=\u001b[39m_filter_safe_paths(archive)\n\u001b[1;32m    181\u001b[0m         )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (tarfile\u001b[38;5;241m.\u001b[39mTarError, \u001b[38;5;167;01mRuntimeError\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/tarfile.py:2059\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# Do not set_attrs directories, as we will do that further down\u001b[39;00m\n\u001b[0;32m-> 2059\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract(tarinfo, path, set_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir(),\n\u001b[1;32m   2060\u001b[0m                  numeric_owner\u001b[38;5;241m=\u001b[39mnumeric_owner)\n\u001b[1;32m   2062\u001b[0m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/tarfile.py:2100\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_member(tarinfo, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, tarinfo\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m   2101\u001b[0m                          set_attrs\u001b[38;5;241m=\u001b[39mset_attrs,\n\u001b[1;32m   2102\u001b[0m                          numeric_owner\u001b[38;5;241m=\u001b[39mnumeric_owner)\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/tarfile.py:2173\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misreg():\n\u001b[0;32m-> 2173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakefile(tarinfo, targetpath)\n\u001b[1;32m   2174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/tarfile.py:2214\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2213\u001b[0m bufsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopybufsize\n\u001b[0;32m-> 2214\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m bltn_open(targetpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m target:\n\u001b[1;32m   2215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39msparse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m current_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[1;32m      4\u001b[0m     fname \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maclImdb.tar.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      5\u001b[0m     origin \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39m  current_folder,\n\u001b[1;32m      7\u001b[0m     extract \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages/keras/utils/data_utils.py:374\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untar_fpath\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extract:\n\u001b[0;32m--> 374\u001b[0m     _extract_archive(fpath, datadir, archive_format)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fpath\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages/keras/utils/data_utils.py:187\u001b[0m, in \u001b[0;36m_extract_archive\u001b[0;34m(file_path, path, archive_format)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 os\u001b[38;5;241m.\u001b[39mremove(path)\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m                 shutil\u001b[38;5;241m.\u001b[39mrmtree(path)\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/shutil.py:731\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror, dir_fd)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msamestat(orig_st, os\u001b[38;5;241m.\u001b[39mfstat(fd)):\n\u001b[0;32m--> 731\u001b[0m         _rmtree_safe_fd(fd, path, onerror)\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m             os\u001b[38;5;241m.\u001b[39mclose(fd)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/shutil.py:659\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msamestat(orig_st, os\u001b[38;5;241m.\u001b[39mfstat(dirfd)):\n\u001b[0;32m--> 659\u001b[0m         _rmtree_safe_fd(dirfd, fullname, onerror)\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m             os\u001b[38;5;241m.\u001b[39mclose(dirfd)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/shutil.py:659\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msamestat(orig_st, os\u001b[38;5;241m.\u001b[39mfstat(dirfd)):\n\u001b[0;32m--> 659\u001b[0m         _rmtree_safe_fd(dirfd, fullname, onerror)\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m             os\u001b[38;5;241m.\u001b[39mclose(dirfd)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/shutil.py:659\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msamestat(orig_st, os\u001b[38;5;241m.\u001b[39mfstat(dirfd)):\n\u001b[0;32m--> 659\u001b[0m         _rmtree_safe_fd(dirfd, fullname, onerror)\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m             os\u001b[38;5;241m.\u001b[39mclose(dirfd)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/shutil.py:680\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 680\u001b[0m         os\u001b[38;5;241m.\u001b[39munlink(entry\u001b[38;5;241m.\u001b[39mname, dir_fd\u001b[38;5;241m=\u001b[39mtopfd)\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    682\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39munlink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_folder = os.getcwd()\n",
    " \n",
    "dataset = tf.keras.utils.get_file(\n",
    "    fname =\"aclImdb.tar.gz\", \n",
    "    origin =\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "    cache_dir=  current_folder,\n",
    "    extract = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc9ea3-9219-4a28-a202-5fb8756f98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.dirname(dataset)\n",
    "dataset_dir = os.path.join(dataset_path, 'aclImdb')\n",
    "train_dir = os.path.join(dataset_dir,'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab89d9a-6ed6-4d4d-be67-9dc4a02a6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory):\n",
    "    data = {\"sentence\": [], \"sentiment\": []}\n",
    "    for file_name in os.listdir(directory):\n",
    "        #print(file_name)\n",
    "        if file_name == 'pos':\n",
    "            positive_dir = os.path.join(directory, file_name)\n",
    "            for text_file in os.listdir(positive_dir):\n",
    "                text = os.path.join(positive_dir, text_file)\n",
    "                with open(text, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data[\"sentence\"].append(f.read())\n",
    "                    data[\"sentiment\"].append(1)\n",
    "        elif file_name == 'neg':\n",
    "            negative_dir = os.path.join(directory, file_name)\n",
    "            for text_file in os.listdir(negative_dir):\n",
    "                text = os.path.join(negative_dir, text_file)\n",
    "                with open(text, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data[\"sentence\"].append(f.read())\n",
    "                    data[\"sentiment\"].append(0)\n",
    "             \n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac8847-9165-4c35-8719-b8de0f9c97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(train_dir)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5cc36-2f75-461e-9dc0-b205989bcaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 12499\n",
    "print(train_df.iloc[num,1])\n",
    "train_df.iloc[num,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3dee0-d811-454b-8305-32106e1ba591",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(dataset_dir,'test')\n",
    "test_df = load_dataset(test_dir)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f11b52-b60c-4d46-b1ba-cb34be0f8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = train_df['sentiment'].value_counts()\n",
    " \n",
    "fig =px.bar(x= {0:'Negative',1:'Positive'},\n",
    "            y= sentiment_counts.values,\n",
    "            color=sentiment_counts.index,\n",
    "            color_discrete_sequence =  px.colors.qualitative.Dark24,\n",
    "            title='<b>Sentiments Counts')\n",
    " \n",
    "fig.update_layout(title='Sentiments Counts',\n",
    "                  xaxis_title='Sentiment',\n",
    "                  yaxis_title='Counts',\n",
    "                  template='plotly_dark')\n",
    " \n",
    "# Show the bar chart\n",
    "fig.show()\n",
    "#pyo.plot(fig, filename = 'Sentiments Counts.html', auto_open = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa28c3d-83a4-4738-b2a5-4998c9454147",
   "metadata": {},
   "outputs": [],
   "source": [
    "   def text_cleaning(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = re.sub(r'\\[[^]]*\\]', '', soup.get_text())\n",
    "    pattern = r\"[^a-zA-Z0-9\\s,']\"\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038f565-0721-4e5a-b12f-138c2d1aa6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_df['Cleaned_sentence'] = train_df['sentence'].apply(text_cleaning).tolist()\n",
    "# Test dataset\n",
    "test_df['Cleaned_sentence'] = test_df['sentence'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b39c10-b480-438c-a9e7-e9fe7be8ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(text,Title):\n",
    "    all_text = \" \".join(text)\n",
    "    wordcloud = WordCloud(width=800, \n",
    "                          height=400,\n",
    "                          stopwords=set(STOPWORDS), \n",
    "                          background_color='black').generate(all_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(Title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d205593-9ebb-48ad-b9ce-924ad30f9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = train_df[train_df['sentiment']==1]['Cleaned_sentence'].tolist()\n",
    "generate_wordcloud(positive,'Positive Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64b603-2e26-4014-9b73-efc05e8f9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = train_df[train_df['sentiment']==0]['Cleaned_sentence'].tolist()\n",
    "generate_wordcloud(negative,'Negative Review')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae394f-a097-453a-9855-815793a22099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "#Reviews = \"[CLS] \" +train_df['Cleaned_sentence'] + \"[SEP]\"\n",
    "Reviews = train_df['Cleaned_sentence']\n",
    "Target = train_df['sentiment']\n",
    " \n",
    "# Test data\n",
    "#test_reviews =  \"[CLS] \" +test_df['Cleaned_sentence'] + \"[SEP]\"\n",
    "test_reviews = test_df['Cleaned_sentence']\n",
    "test_targets = test_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab86be-4e08-456c-b747-8278ea0bd6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    " x_val, x_test, y_val, y_test = train_test_split(test_reviews,\n",
    "                                                    test_targets,\n",
    "                                                    test_size=0.5, \n",
    "                                                    stratify = test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c325f3-d550-450b-897f-8b470ab6b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3554173c-9cc9-4dc3-a598-81f51acfd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len= 128\n",
    "# Tokenize and encode the sentences\n",
    "X_train_encoded = tokenizer.batch_encode_plus(Reviews.tolist(),\n",
    "                                              padding=True, \n",
    "                                              truncation=True,\n",
    "                                              max_length = max_len,\n",
    "                                              return_tensors='tf')\n",
    " \n",
    "X_val_encoded = tokenizer.batch_encode_plus(x_val.tolist(), \n",
    "                                              padding=True, \n",
    "                                              truncation=True,\n",
    "                                              max_length = max_len,\n",
    "                                              return_tensors='tf')\n",
    " \n",
    "X_test_encoded = tokenizer.batch_encode_plus(x_test.tolist(), \n",
    "                                              padding=True, \n",
    "                                              truncation=True,\n",
    "                                              max_length = max_len,\n",
    "                                              return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5d450-ab55-4209-a500-89f6576ab285",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0635a-c79d-432a-a40c-cff50b170086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with an appropriate optimizer, loss function, and metrics\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec3b82-7c7c-4962-9f3f-a685a77da694",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [X_train_encoded['input_ids'], X_train_encoded['token_type_ids'], X_train_encoded['attention_mask']],\n",
    "    Target,\n",
    "    validation_data=(\n",
    "      [X_val_encoded['input_ids'], X_val_encoded['token_type_ids'], X_val_encoded['attention_mask']],y_val),\n",
    "    batch_size=32,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa19cc-e195-47f6-b227-7437eb6c9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(\n",
    "    [X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']],\n",
    "    y_test\n",
    ")\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2096a-6292-433b-9bb1-0ae29c876821",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'geeksforgeeks_example')\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(path +'/Tokenizer')\n",
    " \n",
    "# Save model\n",
    "model.save_pretrained(path +'/Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08730d86-d6c6-45a6-8dc6-df48fe912016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(path +'/Tokenizer')\n",
    " \n",
    "# Load model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(path +'/Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c394c76-5088-4fa0-ae96-f98c17d1a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = bert_model.predict(\n",
    "    [X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']])\n",
    " \n",
    "# pred is of type TFSequenceClassifierOutput\n",
    "logits = pred.logits\n",
    " \n",
    "# Use argmax along the appropriate axis to get the predicted labels\n",
    "pred_labels = tf.argmax(logits, axis=1)\n",
    " \n",
    "# Convert the predicted labels to a NumPy array\n",
    "pred_labels = pred_labels.numpy()\n",
    " \n",
    "label = {\n",
    "    1: 'positive',\n",
    "    0: 'Negative'\n",
    "}\n",
    " \n",
    "# Map the predicted labels to their corresponding strings using the label dictionary\n",
    "pred_labels = [label[i] for i in pred_labels]\n",
    "Actual = [label[i] for i in y_test]\n",
    " \n",
    "print('Predicted Label :', pred_labels[:10])\n",
    "print('Actual Label    :', Actual[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39726bc-8013-48e1-8498-8b02e2adfd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report: \\n\", classification_report(Actual, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833df962-f69c-4dcf-9dc9-a7bd68f5e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def Get_sentiment(Review, Tokenizer=bert_tokenizer, Model=bert_model):\n",
    "    # Convert Review to a list if it's not already a list\n",
    "    if not isinstance(Review, list):\n",
    "        Review = [Review]\n",
    " \n",
    "    Input_ids, Token_type_ids, Attention_mask = Tokenizer.batch_encode_plus(Review,\n",
    "                                                                             padding=True,\n",
    "                                                                             truncation=True,\n",
    "                                                                             max_length=128,\n",
    "                                                                             return_tensors='tf').values()\n",
    "    prediction = Model.predict([Input_ids, Token_type_ids, Attention_mask])\n",
    " \n",
    "    # Use argmax along the appropriate axis to get the predicted labels\n",
    "    pred_labels = tf.argmax(prediction.logits, axis=1)\n",
    " \n",
    "    # Convert the TensorFlow tensor to a NumPy array and then to a list to get the predicted sentiment labels\n",
    "    pred_labels = [label[i] for i in pred_labels.numpy().tolist()]\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e5ceb-e71c-495d-a228-100137749883",
   "metadata": {},
   "outputs": [],
   "source": [
    "Review ='''Bahubali is a blockbuster Indian movie that was released in 2015. \n",
    "It is the first part of a two-part epic saga that tells the story of a legendary hero who fights for his kingdom and his love. \n",
    "The movie has received rave reviews from critics and audiences alike for its stunning visuals, \n",
    "spectacular action scenes, and captivating storyline.'''\n",
    "Get_sentiment(Review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn-env]",
   "language": "python",
   "name": "conda-env-sklearn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
