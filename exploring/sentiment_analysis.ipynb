{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57b7666-214b-474a-adbc-542e72d94066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c505db-aaed-4d3f-908d-537df4352fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time (ET)</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Favorite</th>\n",
       "      <th>Score</th>\n",
       "      <th>Spread</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Notes</th>\n",
       "      <th>GameHub/RedZone</th>\n",
       "      <th>Game thread link?</th>\n",
       "      <th>Post-game thread link?</th>\n",
       "      <th>Game thread</th>\n",
       "      <th>Post game thread</th>\n",
       "      <th>GameHub Scraped</th>\n",
       "      <th>Game thread scraped</th>\n",
       "      <th>Post game thread Scraped</th>\n",
       "      <th>Subscribers (in thousands)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Thu</td>\n",
       "      <td>2017-09-07</td>\n",
       "      <td>8:30</td>\n",
       "      <td>@</td>\n",
       "      <td>New England Patriots</td>\n",
       "      <td>L 27-42</td>\n",
       "      <td>L -8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6yr619/g...</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6ysc10/p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3193, ('Dear Falcons fans: are you guys ready...</td>\n",
       "      <td>[2781, ('I\\'m imagining a world where the Patr...</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>1:00</td>\n",
       "      <td>@</td>\n",
       "      <td>Buffalo Bills</td>\n",
       "      <td>W 21-12</td>\n",
       "      <td>W -7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6z90qu/g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6z97il/g...</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6zajs1/p...</td>\n",
       "      <td>[240, ('Bitter-sweet start to the NFL season w...</td>\n",
       "      <td>[109, ('I\\'m a Jets fan. I\\'m a Texas A&amp;M fan....</td>\n",
       "      <td>[120, ('Bills number 1 seed in the AFCE!', 194...</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>1:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta Falcons</td>\n",
       "      <td>W 23-17</td>\n",
       "      <td>L -6.5</td>\n",
       "      <td>@</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6z90qu/g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6z97gn/g...</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6zajuc/p...</td>\n",
       "      <td>[240, ('Bitter-sweet start to the NFL season w...</td>\n",
       "      <td>[321, ('Wow this national anthem singer is a f...</td>\n",
       "      <td>[351, (\"We just went down to the wire against ...</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>1:00</td>\n",
       "      <td>@</td>\n",
       "      <td>Cincinnati Bengals</td>\n",
       "      <td>L 0-20</td>\n",
       "      <td>L -2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6z90qu/g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6z97hl/g...</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6zajr0/p...</td>\n",
       "      <td>[240, ('Bitter-sweet start to the NFL season w...</td>\n",
       "      <td>[192, (\"Man I hope Woodhead stays healthy for ...</td>\n",
       "      <td>[293, ('[Summary of this game](https://pbs.twi...</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>1:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pittsburgh Steelers</td>\n",
       "      <td>W 21-18</td>\n",
       "      <td>L -10</td>\n",
       "      <td>@</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6z90qu/g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6z97l9/g...</td>\n",
       "      <td>https://www.reddit.com/r/nfl/comments/6zajwi/p...</td>\n",
       "      <td>[240, ('Bitter-sweet start to the NFL season w...</td>\n",
       "      <td>[367, (\"1 drive no turnovers. WE'VE TURNED THE...</td>\n",
       "      <td>[383, (\"Don't let this distract you from the f...</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  Week  Day       Date Time (ET) Unnamed: 5              Favorite  \\\n",
       "0    2017     1  Thu 2017-09-07      8:30          @  New England Patriots   \n",
       "1    2017     1  Sun 2017-09-10      1:00          @         Buffalo Bills   \n",
       "2    2017     1  Sun 2017-09-10      1:00        NaN       Atlanta Falcons   \n",
       "3    2017     1  Sun 2017-09-10      1:00          @    Cincinnati Bengals   \n",
       "4    2017     1  Sun 2017-09-10      1:00        NaN   Pittsburgh Steelers   \n",
       "\n",
       "     Score  Spread Unnamed: 9  ... Notes  \\\n",
       "0  L 27-42    L -8        NaN  ...   NaN   \n",
       "1  W 21-12    W -7        NaN  ...   NaN   \n",
       "2  W 23-17  L -6.5          @  ...   NaN   \n",
       "3   L 0-20  L -2.5        NaN  ...   NaN   \n",
       "4  W 21-18   L -10          @  ...   NaN   \n",
       "\n",
       "                                     GameHub/RedZone Game thread link?  \\\n",
       "0                                                NaN               NaN   \n",
       "1  https://www.reddit.com/r/nfl/comments/6z90qu/g...               NaN   \n",
       "2  https://www.reddit.com/r/nfl/comments/6z90qu/g...               NaN   \n",
       "3  https://www.reddit.com/r/nfl/comments/6z90qu/g...               NaN   \n",
       "4  https://www.reddit.com/r/nfl/comments/6z90qu/g...               NaN   \n",
       "\n",
       "  Post-game thread link?                                        Game thread  \\\n",
       "0                    NaN  https://www.reddit.com/r/nfl/comments/6yr619/g...   \n",
       "1                    1.0  https://www.reddit.com/r/nfl/comments/6z97il/g...   \n",
       "2                    1.0  https://www.reddit.com/r/nfl/comments/6z97gn/g...   \n",
       "3                    1.0  https://www.reddit.com/r/nfl/comments/6z97hl/g...   \n",
       "4                    1.0  https://www.reddit.com/r/nfl/comments/6z97l9/g...   \n",
       "\n",
       "                                    Post game thread  \\\n",
       "0  https://www.reddit.com/r/nfl/comments/6ysc10/p...   \n",
       "1  https://www.reddit.com/r/nfl/comments/6zajs1/p...   \n",
       "2  https://www.reddit.com/r/nfl/comments/6zajuc/p...   \n",
       "3  https://www.reddit.com/r/nfl/comments/6zajr0/p...   \n",
       "4  https://www.reddit.com/r/nfl/comments/6zajwi/p...   \n",
       "\n",
       "                                     GameHub Scraped  \\\n",
       "0                                                NaN   \n",
       "1  [240, ('Bitter-sweet start to the NFL season w...   \n",
       "2  [240, ('Bitter-sweet start to the NFL season w...   \n",
       "3  [240, ('Bitter-sweet start to the NFL season w...   \n",
       "4  [240, ('Bitter-sweet start to the NFL season w...   \n",
       "\n",
       "                                 Game thread scraped  \\\n",
       "0  [3193, ('Dear Falcons fans: are you guys ready...   \n",
       "1  [109, ('I\\'m a Jets fan. I\\'m a Texas A&M fan....   \n",
       "2  [321, ('Wow this national anthem singer is a f...   \n",
       "3  [192, (\"Man I hope Woodhead stays healthy for ...   \n",
       "4  [367, (\"1 drive no turnovers. WE'VE TURNED THE...   \n",
       "\n",
       "                            Post game thread Scraped  \\\n",
       "0  [2781, ('I\\'m imagining a world where the Patr...   \n",
       "1  [120, ('Bills number 1 seed in the AFCE!', 194...   \n",
       "2  [351, (\"We just went down to the wire against ...   \n",
       "3  [293, ('[Summary of this game](https://pbs.twi...   \n",
       "4  [383, (\"Don't let this distract you from the f...   \n",
       "\n",
       "  Subscribers (in thousands)  \n",
       "0                        640  \n",
       "1                        640  \n",
       "2                        640  \n",
       "3                        640  \n",
       "4                        640  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with zipfile.ZipFile(\"../data/reddit_scrape.csv.zip\") as z:\n",
    "    with z.open(\"reddit_scrape.csv\") as f:\n",
    "        reddit_df = pd.read_csv(f)\n",
    "\n",
    "reddit_df['Date'] = pd.to_datetime(reddit_df['Date'])    \n",
    "\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b970f37-39db-42b7-ab62-85c0737ee01c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scrape_cols = [x for x in reddit_df.columns if 'craped' in x]\n",
    "for col in scrape_cols: # converts strings to lists\n",
    "    reddit_df[col] = reddit_df[col].apply(lambda x: eval(x) if type(x)==str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9772ce21-6e95-445e-a475-5a8d4667ddb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Falcons fans: are you guys ready for 17 straight weeks of hearing how you fucked up the Super Bowl? I feel your pain. \\n\\nSincerely, \\n\\n\\nSeahawks fans everywhere.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = reddit_df['Game thread scraped'][0][1][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "810ee356-ef80-412e-ab39-6e744c46b7ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Falcons fans: are you guys ready for 17 straight weeks of hearing how you fucked up the Super Bowl? I feel your pain. Sincerely, Seahawks fans everywhere.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from markdown import Markdown\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def unmark_element(element, stream=None):\n",
    "    if stream is None:\n",
    "        stream = StringIO()\n",
    "    if element.text:\n",
    "        stream.write(element.text)\n",
    "    for sub in element:\n",
    "        unmark_element(sub, stream)\n",
    "    if element.tail:\n",
    "        stream.write(element.tail)\n",
    "    return stream.getvalue()\n",
    "\n",
    "\n",
    "# patching Markdown\n",
    "Markdown.output_formats[\"plain\"] = unmark_element\n",
    "__md = Markdown(output_format=\"plain\")\n",
    "__md.stripTopLevelTags = False\n",
    "\n",
    "\n",
    "def unmark(text):\n",
    "    return __md.convert(text).replace(\" \\n\",\" \")\n",
    "\n",
    "text = unmark(text).replace(\" \\n\",\" \")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "270f197d-eaf5-4f9f-a449-e163043dd3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Falcons fans are you guys ready for 17 straight weeks of hearing how you fucked up the Super Bowl I feel your pain Sincerely Seahawks fans everywhere'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d348f4ad-c29f-41b4-86e6-f2d50858d561",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages (from vaderSentiment) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages (from requests->vaderSentiment) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/sklearn-env/lib/python3.11/site-packages (from requests->vaderSentiment) (2023.11.17)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bc94fff5-0b4f-40e5-912e-ec836ec09015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5212fb01-3e98-4894-80b3-21b5ddb68ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sentiment_scores(sentence):\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # oject gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "    print(\"Sentence Overall Rated As\", end = \" \")\n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        print(\"Positive\")\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        print(\"Negative\")\n",
    "    else :\n",
    "        print(\"Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7cf38b27-677a-44b0-96bc-d44d7bf60b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentiment dictionary is :  {'neg': 0.18, 'neu': 0.492, 'pos': 0.328, 'compound': 0.6486}\n",
      "sentence was rated as  18.0 % Negative\n",
      "sentence was rated as  49.2 % Neutral\n",
      "sentence was rated as  32.800000000000004 % Positive\n",
      "Sentence Overall Rated As Positive\n"
     ]
    }
   ],
   "source": [
    "sentiment_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6880eb6b-21df-45f6-8ee1-cb8798c600f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fuckin Mark Wahlberg LEFT THE SUPER BOWL EARLY BECAUSE THE PATS WERE DOWN 28-3\n",
      "Overall sentiment dictionary is :  {'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.6841}\n",
      "sentence was rated as  0.0 % Negative\n",
      "sentence was rated as  73.7 % Neutral\n",
      "sentence was rated as  26.3 % Positive\n",
      "Sentence Overall Rated As Positive\n"
     ]
    }
   ],
   "source": [
    "text = reddit_df['Game thread scraped'][0][2][0]\n",
    "print(text)\n",
    "text = unmark(text).replace(\" \\n\",\" \")\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "sentiment_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9daea5bb-5f7f-49ca-943d-f4313ca6df89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SuperBowl is a great event.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_super_bowl(text):\n",
    "    # Use regular expression to find and replace \"super bowl\" with \"superbowl\" (case-insensitive)\n",
    "    modified_text = re.sub(r'\\b(super)\\s+(bowl)\\b', r'\\1\\2', text, flags=re.IGNORECASE)\n",
    "    return modified_text\n",
    "\n",
    "# Example usage:\n",
    "original_text = \"The Super Bowl is a great event.\"\n",
    "converted_text = convert_super_bowl(original_text)\n",
    "print(converted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c81fda98-6cc0-4330-82ad-98e5701741d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brady drafted in the 6th round...\n",
      "\n",
      "Minshew II drafted in the 6th round....\n",
      "\n",
      "\n",
      "I’m starting to see a trend here.\n",
      "____________________________________________________________________________________________________\n",
      "Positive: []\n",
      "Neutral: ['Brady', 'drafted', 'in', 'the', '6th', 'round', 'Minshew', 'II', 'drafted', 'in', 'the', '6th', 'round', 'I', '’', 'm', 'starting', 'to', 'see', 'a', 'trend', 'here']\n",
      "Negative: []\n",
      "\n",
      "Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} \n",
      "\n",
      "Sentence Overall Rated As Neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/rohanjha/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num_1 = random.randrange(len(reddit_df))\n",
    "num_2 = random.randrange(len(reddit_df.iloc[num_1,:].loc['Game thread scraped']))\n",
    "text = reddit_df['Game thread scraped'][num_1][num_2][0]\n",
    "print(text)\n",
    "print(\"____________________________________________________________________________________________________\")\n",
    "text = unmark(text).replace(\" \\n\",\" \")\n",
    "text = convert_super_bowl(text)\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "tokenized_sentence = nltk.word_tokenize(text)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        neg_word_list.append(word)\n",
    "    else:\n",
    "        neu_word_list.append(word)                \n",
    "\n",
    "print('Positive:',pos_word_list)        \n",
    "print('Neutral:',neu_word_list)    \n",
    "print('Negative:',neg_word_list) \n",
    "score = sid.polarity_scores(text)\n",
    "print('\\nScores:', score,\"\\n\")\n",
    "print(\"Sentence Overall Rated As\", end = \" \")\n",
    "if score['compound'] >= 0.05 :\n",
    "    print(\"Positive\")\n",
    "elif score['compound'] <= - 0.05 :\n",
    "    print(\"Negative\")\n",
    "else :\n",
    "    print(\"Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3cf057b8-deb1-4cf0-bec9-3d066fda5e30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rohanjha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1d938155-ad86-45e4-9e04-182cd8e5387d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fuckin',\n",
       " 'Mark',\n",
       " 'Wahlberg',\n",
       " 'LEFT',\n",
       " 'THE',\n",
       " 'SUPER',\n",
       " 'BOWL',\n",
       " 'EARLY',\n",
       " 'BECAUSE',\n",
       " 'THE',\n",
       " 'PATS',\n",
       " 'WERE',\n",
       " 'DOWN',\n",
       " '283']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(text)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "92542450-e288-43be-8965-4e23c27732e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuckin',\n",
       " 'mark',\n",
       " 'wahlberg',\n",
       " 'left',\n",
       " 'the',\n",
       " 'super',\n",
       " 'bowl',\n",
       " 'earli',\n",
       " 'becaus',\n",
       " 'the',\n",
       " 'pat',\n",
       " 'were',\n",
       " 'down',\n",
       " '283']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "stemmed_words = [pst.stem(x) for x in token]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fd65d85e-f2e9-482d-9970-56c80081e962",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rohanjha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fuckin',\n",
       " 'mark',\n",
       " 'wahlberg',\n",
       " 'left',\n",
       " 'the',\n",
       " 'super',\n",
       " 'bowl',\n",
       " 'earli',\n",
       " 'becaus',\n",
       " 'the',\n",
       " 'pat',\n",
       " 'were',\n",
       " 'down',\n",
       " '283']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(x) for x in stemmed_words]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a3ff56ad-c3ff-4964-8925-9113564b6631",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuckin',\n",
       " 'mark',\n",
       " 'wahlberg',\n",
       " 'left',\n",
       " 'super',\n",
       " 'bowl',\n",
       " 'earli',\n",
       " 'becaus',\n",
       " 'pat',\n",
       " '283']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "a = set(stopwords.words('english'))\n",
    "no_stopwords = [x for x in lemmatized_words if x not in a]\n",
    "no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a0d43a2e-f49b-4f5b-8081-616b8ebad1df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rohanjha/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Fuckin', 'NNP')]\n",
      "[('Mark', 'NN')]\n",
      "[('Wahlberg', 'NNP')]\n",
      "[('LEFT', 'NNP')]\n",
      "[('THE', 'DT')]\n",
      "[('SUPER', 'NN')]\n",
      "[('BOWL', 'NN')]\n",
      "[('EARLY', 'JJ')]\n",
      "[('BECAUSE', 'IN')]\n",
      "[('THE', 'DT')]\n",
      "[('PATS', 'NN')]\n",
      "[('WERE', 'NN')]\n",
      "[('DOWN', 'RB')]\n",
      "[('283', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "for word in token:\n",
    "    print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02237dfa-bdef-4e42-a88b-958384816862",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'fan': 2, 'dear': 1, 'falcon': 1, 'guy': 1, 'readi': 1, '17': 1, 'straight': 1, 'week': 1, 'hear': 1, 'fuck': 1, ...})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist([pst.stem(x) for x in no_stopwords])\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c69cea-d209-4777-b52a-2de80a406edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sklearn-env] *",
   "language": "python",
   "name": "conda-env-sklearn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
